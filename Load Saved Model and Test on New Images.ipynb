{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import pickle\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Convert Images to GraySacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert color images into grayscale images\n",
    "def color_2_grayscale(image_data):\n",
    "    gray_image_data = np.zeros(image_data.shape[:-1])\n",
    "    #gray_image_data = np.zeros([len(image_data), image_data[0].shape[0], image_data[0].shape[1], 1])\n",
    "    for i in range(len(image_data)):\n",
    "        gray_image_data[i] = cv2.cvtColor(image_data[i],cv2.COLOR_RGB2GRAY)\n",
    "    gray_image_data =  np.float32(gray_image_data)\n",
    "    gray_image_data = gray_image_data.reshape(len(image_data),image_data[0].shape[0], image_data[0].shape[1], 1)\n",
    "    return gray_image_data\n",
    "\n",
    "#X_test = color_2_grayscale(X_test)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Normalize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize grayscale images between [-1, 1], to have 0 mean and unit standard deviation\n",
    "def normalize_grayscale(image_data):\n",
    "    img_min = 0\n",
    "    img_max = 255\n",
    "    new_min = -1\n",
    "    new_max = 1\n",
    "    x1 = (image_data - img_min) / (img_max - img_min)\n",
    "    x2 = x1 * (new_max - new_min)\n",
    "    x2 = x2 + new_min\n",
    "    return x2\n",
    "\n",
    "#X_test = normalize_grayscale(X_test)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and Biases Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "rate = 0.0008\n",
    "n_classes = 43\n",
    "\n",
    "mu = 0 # mean of weights\n",
    "sigma = 0.1 # standard deviation of weights\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 6], mu, sigma), name = \"W1\")# [height, width, input_depth, output_depth]\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, 6, 16], mu, sigma), name = \"W2\") # [height, width, input_depth, output_depth]\n",
    "W3 = tf.Variable(tf.truncated_normal([5*5*16, 120], mu, sigma), name = \"W3\") # [input_features, output_features]\n",
    "W4 = tf.Variable(tf.truncated_normal([120, 84], mu, sigma), name = \"W4\") # [input_features, output_classes]\n",
    "W5 = tf.Variable(tf.truncated_normal([84, n_classes], mu, sigma), name = \"W5\") # [input_features, output_classes]\n",
    "\n",
    "b1 = tf.Variable(tf.zeros(6), name = \"b1\") # depth of convolution\n",
    "b2 = tf.Variable(tf.zeros(16), name = \"b2\") # depth of convolution\n",
    "b3 = tf.Variable(tf.zeros(120), name = \"b3\") # number of output features in fully connected layer\n",
    "b4 = tf.Variable(tf.zeros(84), name = \"b4\") # number of classes in output layer           \n",
    "b5 = tf.Variable(tf.zeros(n_classes), name = \"b5\") # number of classes in output layer                     \n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1)) \n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "dropout = tf.placeholder(tf.float32) \n",
    "one_hot_y = tf.one_hot(y, n_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LeNet \n",
    "def Modified_LeNet(x, dropout):    \n",
    "    strides = 1\n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x16, i.e., (H: (32 - 5 + 1)/1, W =(32 - 5 + 1)/1)\n",
    "    conv1 = tf.nn.conv2d(x, W1, strides=[1, strides, strides, 1], padding='VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, b1)\n",
    "    # TODO: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    # TODO: Pooling. Input = 28x28x16. Output = 14x14x16.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #print(conv1)\n",
    "    # TODO: Layer 2: Convolutional Input = 14x14x16 Output = 10x10x32.\n",
    "    conv2 = tf.nn.conv2d(conv1, W2, strides=[1, strides, strides, 1], padding='VALID')\n",
    "    conv2 = tf.nn.bias_add(conv2, b2)\n",
    "    # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # TODO: Pooling. Input = 10x10x32. Output = 5x5x32.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #print(conv2)\n",
    "    # TODO: Flatten. Input = 5x5x32. Output = 800.\n",
    "    fc1 = tf.reshape(conv2, [-1, W3.get_shape().as_list()[0]])\n",
    "    # TODO: Layer 3: Fully Connected. Input = 800. Output = 120.\n",
    "    fc1 = tf.add(tf.matmul(fc1,W3), b3)\n",
    "    # TODO: Activation.\n",
    "    fc1 = tf.nn.relu(fc1) \n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    #print(fc1)\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2 = tf.add(tf.matmul(fc1, W4), b4)\n",
    "    # TODO: Activation.\n",
    "    fc2 = tf.nn.relu(fc2) \n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "    #print(fc2)\n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = n_classes.\n",
    "    fc3 = tf.add(tf.matmul(fc2, W5), b5)\n",
    "    #print(fc3)\n",
    "    logits = fc3\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions to Predict the Labels and Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "beta = 0.0001\n",
    "# we set up our training pipeline here to train the model\n",
    "logits = Modified_LeNet(x, dropout) # pass the input data to LeNet() to calculate our logits\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits) # tf.nn.softmax_cross_entropy_with_logits() \n",
    "#function is used to compare those logits to the ground truth labels and caculate the cross entropy\n",
    "# cross entropy is a measure of how different the logits are from the ground truth training labels\n",
    "\n",
    "# Loss function using L2 Regularization\n",
    "loss_operation = tf.reduce_mean(cross_entropy + beta * tf.nn.l2_loss(W1) + beta * tf.nn.l2_loss(W2) +\n",
    "    beta * tf.nn.l2_loss(W3) + beta * tf.nn.l2_loss(W4) + beta * tf.nn.l2_loss(W5)) \n",
    "loss_operation1 = tf.reduce_mean(cross_entropy) \n",
    "# tf.reduce_mean() function is used calculate the average cross entropy from all the training images\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate) # tf.train.AdamOptimizer() function is used to minimize the loss \n",
    "# function similarly to what stochastic gradient descent does. The Adam algorithm is a little more sophisticated than stochastic\n",
    "# gradeient descent, so its a good defualt choice for an optimizer. This where we use learning rate hyper-parameter\n",
    "training_operation = optimizer.minimize(loss_operation) # we run the minimize function on the optimizer which uses backpropagation\n",
    "# to update the network and minimize our training loss\n",
    "\n",
    "# measure whether a given prediction is correct by comparing the logit prediction ot the one-hot encoded ground truth label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "# calculate the model's overall accuracy by averaging the individual prediciton accuracies\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# build evaluate functions to evaluate accuracy and loss\n",
    "def evaluate(X_data, y_data, prob): # dataset as input\n",
    "    num_examples = len(X_data) # number of samples in X_data\n",
    "    total_accuracy = 0 \n",
    "    total_loss = 0\n",
    "    sess = tf.get_default_session()\n",
    "    # batches the dataset and runs it through the evaluation pipeline for accuracy \n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        loss, accuracy = sess.run([loss_operation1, accuracy_operation], feed_dict={x: batch_x, y: batch_y, dropout: prob})\n",
    "        total_accuracy += (accuracy * len(batch_x)) # add total accuracies for all the batches\n",
    "        total_loss += (loss * len(batch_x))  # add total losses for all the batches\n",
    "    accu = total_accuracy / num_examples\n",
    "    los = total_loss / num_examples\n",
    "    return los, accu  # returns the average accuracy and loss for the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of New Images\n",
      "(10, 32, 32, 1)\n",
      "\n",
      "List of Labels of New Images\n",
      "[36 15 34 26 18 12 16 14 17 13]\n"
     ]
    }
   ],
   "source": [
    "input = 'test_images/*.jpg'\n",
    "\n",
    "name = glob.glob(input)\n",
    "X_new = [ ]\n",
    "y_new = [ ]\n",
    "for image in name:  \n",
    "    #print(image)\n",
    "    img_name = image.split('_')[2] \n",
    "    label = img_name.split('.')[0] \n",
    "    y_new.append(np.uint8(label))\n",
    "    img = mpimg.imread(image)\n",
    "    X_new.append(img)\n",
    "\n",
    "X_new = np.array(X_new)\n",
    "y_new = np.array(y_new)\n",
    "\n",
    "# conver to gray scale\n",
    "X_new = color_2_grayscale(X_new)\n",
    "# normalize images\n",
    "X_new = normalize_grayscale(X_new)\n",
    "print('Shape of New Images')\n",
    "print(X_new.shape)\n",
    "print()\n",
    "print('List of Labels of New Images')\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Label of New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Modified_LeNet.ckpt\n",
      "Actual Label is 36 and Predicted Label is 36\n",
      "Actual Label is 15 and Predicted Label is 15\n",
      "Actual Label is 34 and Predicted Label is 34\n",
      "Actual Label is 26 and Predicted Label is 26\n",
      "Actual Label is 18 and Predicted Label is 18\n",
      "Actual Label is 12 and Predicted Label is 12\n",
      "Actual Label is 16 and Predicted Label is 7\n",
      "Actual Label is 14 and Predicted Label is 14\n",
      "Actual Label is 17 and Predicted Label is 17\n",
      "Actual Label is 13 and Predicted Label is 13\n"
     ]
    }
   ],
   "source": [
    "save_file = './Modified_LeNet.ckpt'\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    labels = sess.run(logits, feed_dict={x: X_new, y: y_new, dropout: 1.0})\n",
    "\n",
    "for i in range(10):\n",
    "    print('Actual Label is', y_new[i], 'and Predicted Label is', np.argmax(labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy of Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Modified_LeNet.ckpt\n",
      "Accuracy of Prediction for New Images = 90.00\n"
     ]
    }
   ],
   "source": [
    "### Calculate the accuracy for new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    accuracy = sess.run(accuracy_operation, feed_dict={x: X_new, y: y_new, dropout: 1.0})\n",
    "\n",
    "print('Accuracy of Prediction for New Images = {:.2f}'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Modified_LeNet.ckpt\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 36\n",
      "[100.   0.   0.   0.   0.]\n",
      "[36 38 32 41 13]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 15\n",
      "[ 99.91   0.06   0.02   0.01   0.  ]\n",
      "[15 17 12  9 13]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 34\n",
      "[ 100.    0.    0.    0.    0.]\n",
      "[34 35 14 38 30]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 26\n",
      "[ 100.    0.    0.    0.    0.]\n",
      "[26 18 24 25  4]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 18\n",
      "[ 100.    0.    0.    0.    0.]\n",
      "[18 26 25 24 22]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 12\n",
      "[ 100.    0.    0.    0.    0.]\n",
      "[12 13  7 40 38]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 16\n",
      "[ 80.26  16.49   3.25   0.     0.  ]\n",
      "[ 7  5 16 42 10]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 14\n",
      "[ 91.17   5.34   2.51   0.6    0.16]\n",
      "[14  2 38 25 21]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 17\n",
      "[ 58.02  41.65   0.31   0.01   0.01]\n",
      "[17  9 34 41 13]\n",
      "\n",
      "Top 5 Softmax Probabilies and their Labels for Image with Label 13\n",
      "[ 100.    0.    0.    0.    0.]\n",
      "[13 12  9 35 15]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    labels = sess.run(logits, feed_dict={x: X_new, y: y_new, dropout: 1.0})\n",
    "    top_five_prob = sess.run(tf.nn.top_k(tf.nn.softmax(labels), k=5))\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "for i in range(top_five_prob[0].shape[0]):\n",
    "    print('Top 5 Softmax Probabilies and their Labels for Image with Label', y_new[i])\n",
    "    print(top_five_prob[0][i] * 100)\n",
    "    print(top_five_prob[1][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
